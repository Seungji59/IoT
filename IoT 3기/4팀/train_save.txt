# train_nextdose.py
# -*- coding: utf-8 -*-
"""
í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìš´ì˜ í”¼í´ ìŠ¤í‚¤ë§ˆì™€ 1:1 í˜¸í™˜)
- onehot: OneHotEncoder(drop_zeros=False, drop_first=False)
- amf   : [MondrianTreeRegressor(...)] * 10  (ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥)
- lin   : StandardScaler -> LinearRegression(SGD 0.01)
- feature_cols: ['hour','minute','dayofweek','hour_sin','hour_cos','minute_sin','minute_cos','delta_same_minutes','ë³µìš©ìˆœì„œ']
- delta_feature: 'delta_same_minutes'
- target_dev = (next_minutes - 1440)

CSV ì˜ˆ)
python train_nextdose.py --csv ë³µì•½ê¸°ë¡_14ì¼_ì‹ì‚¬ê¸°ë°˜.csv \
  --csv-date-col ë‚ ì§œ --csv-time-col ë³µìš©ì‹œê° --csv-order-col ë³µìš©ìˆœì„œ \
  --out nextdose_model.pkl
"""

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional

import numpy as np
import pandas as pd

from river import preprocessing, linear_model, optim, metrics
try:
    from river.tree.mondrian import MondrianTreeRegressor
except Exception:
    from river.tree import MondrianTreeRegressor  # fallback

FEATURE_COLS = [
    "hour","minute","dayofweek",
    "hour_sin","hour_cos","minute_sin","minute_cos",
    "delta_same_minutes","ë³µìš©ìˆœì„œ"
]
DELTA_FEATURE_NAME = "delta_same_minutes"
SEEDS = [1867825,419610,4614226,4108603,3744854,2341057,1719583,9149732,1458591,9906820]

def build_features(now_dt: pd.Timestamp, prev_same_dt: pd.Timestamp, order_val: int) -> Dict[str, Any]:
    hour = now_dt.hour
    minute = now_dt.minute
    feats = {
        "hour": int(hour),
        "minute": int(minute),
        "dayofweek": int(now_dt.dayofweek),
        "hour_sin": float(np.sin(2*np.pi*hour/24)),
        "hour_cos": float(np.cos(2*np.pi*hour/24)),
        "minute_sin": float(np.sin(2*np.pi*minute/60)),
        "minute_cos": float(np.cos(2*np.pi*minute/60)),
        "delta_same_minutes": float((now_dt - prev_same_dt).total_seconds()/60.0),
        "ë³µìš©ìˆœì„œ": int(order_val),
    }
    return feats

def load_csv(csv_path: str, date_col: str, time_col: str, order_col: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    if date_col not in df.columns or time_col not in df.columns or order_col not in df.columns:
        raise SystemExit("CSV ì»¬ëŸ¼ëª…ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    df["ts"] = pd.to_datetime(df[date_col].astype(str) + " " + df[time_col].astype(str), errors="coerce")
    df["ord"] = df[order_col].astype(int)
    df = df.dropna(subset=["ts","ord"]).copy()
    return df[["ts","ord"]].sort_values(["ord","ts"]).reset_index(drop=True)

def make_training_stream(df: pd.DataFrame, clip_abs: float = 180.0) -> List[Tuple[Dict[str, Any], float]]:
    samples: List[Tuple[Dict[str, Any], float]] = []
    for ord_val, g in df.groupby("ord"):
        ts = list(g["ts"].values)
        if len(ts) < 3:
            continue
        for i in range(2, len(ts)):
            prev_prev = pd.Timestamp(ts[i-2])
            prev_now  = pd.Timestamp(ts[i-1])
            nxt       = pd.Timestamp(ts[i])
            x = build_features(prev_now, prev_prev, int(ord_val))
            minutes_ahead = (nxt - prev_now).total_seconds()/60.0
            dev = float(minutes_ahead - 1440.0)
            if clip_abs is not None:
                dev = float(np.clip(dev, -clip_abs, clip_abs))
            samples.append((x, dev))
    return samples

def train_and_dump(samples, out_path: Path, use_ensemble: bool = True,
                   warmup_eval: int = 8, print_each: bool = True):
    onehot = preprocessing.OneHotEncoder(drop_zeros=False, drop_first=False)

    lin = None
    if use_ensemble:
        lin = (preprocessing.StandardScaler(with_std=True) |
               linear_model.LinearRegression(
                   optimizer=optim.SGD(0.01),
                   loss=linear_model.losses.Squared(),
                   l2=0.0))

    forest = [MondrianTreeRegressor(step=1.0, use_aggregation=True, seed=s) for s in SEEDS]

    mae_minutes = metrics.MAE()
    rmse_minutes = metrics.RMSE()

    for idx, (x_raw, y_dev) in enumerate(samples):
        onehot.learn_one(x_raw)
        x_enc = onehot.transform_one(x_raw)

        if idx >= warmup_eval:
            preds = []
            preds_tree = [t.predict_one(x_enc) for t in forest]
            preds_tree = [p for p in preds_tree if p is not None]
            if preds_tree:
                preds.append(sum(preds_tree)/len(preds_tree))
            if lin is not None:
                p_lin = lin.predict_one(x_enc)
                if p_lin is not None:
                    preds.append(p_lin)
            if preds:
                y_pred_dev = float(sum(preds)/len(preds))
                y_true_m = 1440.0 + float(y_dev)
                y_pred_m = 1440.0 + y_pred_dev
                mae_minutes.update(y_true_m, y_pred_m)
                rmse_minutes.update(y_true_m, y_pred_m)
                if print_each:
                    print(f"[{idx:05d}] MAE~ {abs(y_true_m - y_pred_m):4.1f}ë¶„")

        for t in forest:
            t.learn_one(x_enc, y_dev)
        if lin is not None:
            lin.learn_one(x_enc, y_dev)

    print(f"\nğŸ“Œ ìµœì¢… MAE(ë¶„):  {mae_minutes.get():.2f}")
    print(f"ğŸ“Œ ìµœì¢… RMSE(ë¶„): {rmse_minutes.get():.2f}")

    payload = {
        "schema": "v1-amf-list-10",
        "onehot": onehot,
        "amf": forest,   # list of trees
        "lin": lin,
        "use_ensemble": bool(use_ensemble),
        "feature_cols": FEATURE_COLS,
        "delta_feature": DELTA_FEATURE_NAME
    }
    out_path = Path(out_path)
    with open(out_path, "wb") as f:
        pickle.dump(payload, f)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {out_path.resolve()}")

    # quick schema check
    p = pickle.load(open(out_path, "rb"))
    assert isinstance(p["amf"], list) and len(p["amf"]) == 10
    assert "ë³µìš©ìˆœì„œ" in p["feature_cols"]
    print("âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦ OK")

def main():
    ap = argparse.ArgumentParser(description="Train AMF(10)+Linear and dump pickle (schema-compatible)")
    ap.add_argument("--csv", required=True, help="CSV path")
    ap.add_argument("--csv-date-col", default="ë‚ ì§œ")
    ap.add_argument("--csv-time-col", default="ë³µìš©ì‹œê°")
    ap.add_argument("--csv-order-col", default="ë³µìš©ìˆœì„œ")
    ap.add_argument("--clip-abs", type=float, default=180.0)
    ap.add_argument("--out", default="nextdose_model.pkl")
    ap.add_argument("--no-ensemble", action="store_true")
    ap.add_argument("--warmup", type=int, default=8)
    args = ap.parse_args()

    df = load_csv(args.csv, args.csv_date_col, args.csv_time_col, args.csv_order_col)
    stream = make_training_stream(df, clip_abs=args.clip_abs)
    if not stream:
        raise SystemExit("í•™ìŠµ ìƒ˜í”Œì´ ë¶€ì¡±í•©ë‹ˆë‹¤. (orderë³„ ìµœì†Œ 3ê±´ ì´ìƒ í•„ìš”)")

    train_and_dump(stream, Path(args.out), use_ensemble=not args.no_ensemble,
                   warmup_eval=max(0,int(args.warmup)), print_each=True)

if __name__ == "__main__":
    main()
